{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.1 Monte Carlo Methods.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzJ574a6A94",
        "colab_type": "text"
      },
      "source": [
        "**Reinforcement Learning with First Visit Monte Carlo**\n",
        "* This notebook shows how to apply the first visit Monte Carlo to the GridWorld environment\n",
        "\n",
        "\n",
        "Outline:\n",
        "1. Define the GridWorld environment\n",
        "3. Find the value of each Q value in the environment using first visit Monte Carlo\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-RZQ2KOmG9D",
        "colab_type": "text"
      },
      "source": [
        "**GridWorld**\n",
        "\n",
        "The GridWorld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v8rA8v67PKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n",
        "\n",
        "#define the environment\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "import sys\n",
        "from gym.envs.toy_text import discrete\n",
        "import pprint\n",
        "\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "class GridworldEnv(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
        "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
        "    state at the top left or the bottom right corner.\n",
        "    For example, a 4x4 grid looks as follows:\n",
        "    T  o  o  o\n",
        "    o  x  o  o\n",
        "    o  o  o  o\n",
        "    o  o  o  T\n",
        "    x is your position and T are the two terminal states.\n",
        "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
        "    Actions going off the edge leave you in your current state.\n",
        "    You receive a reward of -1 at each step until you reach a terminal state.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def __init__(self, shape=[4,4]):\n",
        "        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
        "            raise ValueError('shape argument must be a list/tuple of length 2')\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "        nS = np.prod(shape)\n",
        "        nA = 4\n",
        "\n",
        "        MAX_Y = shape[0]\n",
        "        MAX_X = shape[1]\n",
        "\n",
        "        P = {}\n",
        "        grid = np.arange(nS).reshape(shape)\n",
        "        it = np.nditer(grid, flags=['multi_index'])\n",
        "\n",
        "        while not it.finished:\n",
        "            s = it.iterindex\n",
        "            y, x = it.multi_index\n",
        "\n",
        "            # P[s][a] = (prob, next_state, reward, is_done)\n",
        "            P[s] = {a : [] for a in range(nA)}\n",
        "\n",
        "            is_done = lambda s: s == 0 or s == (nS - 1)\n",
        "            reward = 0.0 if is_done(s) else -1.0\n",
        "            #reward = 1.0 if is_done(s) else 0.0\n",
        "\n",
        "            # We're stuck in a terminal state\n",
        "            if is_done(s):\n",
        "                P[s][UP] = [(1.0, s, reward, True)]\n",
        "                P[s][RIGHT] = [(1.0, s, reward, True)]\n",
        "                P[s][DOWN] = [(1.0, s, reward, True)]\n",
        "                P[s][LEFT] = [(1.0, s, reward, True)]\n",
        "            # Not a terminal state\n",
        "            else:\n",
        "                ns_up = s if y == 0 else s - MAX_X\n",
        "                ns_right = s if x == (MAX_X - 1) else s + 1\n",
        "                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
        "                ns_left = s if x == 0 else s - 1\n",
        "                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
        "                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
        "                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
        "                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
        "\n",
        "            it.iternext()\n",
        "\n",
        "        # Initial state distribution is uniform\n",
        "        isd = np.ones(nS) / nS\n",
        "\n",
        "        # We expose the model of the environment for educational purposes\n",
        "        # This should not be used in any model-free learning algorithm\n",
        "        self.P = P\n",
        "\n",
        "        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        \"\"\" Renders the current gridworld layout\n",
        "         For example, a 4x4 grid with the mode=\"human\" looks like:\n",
        "            T  o  o  o\n",
        "            o  x  o  o\n",
        "            o  o  o  o\n",
        "            o  o  o  T\n",
        "        where x is your position and T are the two terminal states.\n",
        "        \"\"\"\n",
        "        if close:\n",
        "            return\n",
        "\n",
        "        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "        grid = np.arange(self.nS).reshape(self.shape)\n",
        "        it = np.nditer(grid, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            s = it.iterindex\n",
        "            y, x = it.multi_index\n",
        "\n",
        "            if self.s == s:\n",
        "                output = \" x \"\n",
        "            elif s == 0 or s == self.nS - 1:\n",
        "                output = \" T \"\n",
        "            else:\n",
        "                output = \" o \"\n",
        "\n",
        "            if x == 0:\n",
        "                output = output.lstrip()\n",
        "            if x == self.shape[1] - 1:\n",
        "                output = output.rstrip()\n",
        "\n",
        "            outfile.write(output)\n",
        "\n",
        "            if x == self.shape[1] - 1:\n",
        "                outfile.write(\"\\n\")\n",
        "\n",
        "            it.iternext()\n",
        "            \n",
        "pp = pprint.PrettyPrinter(indent=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azurqbzns_EW",
        "colab_type": "text"
      },
      "source": [
        "**The RL Training Loop**\n",
        "\n",
        "In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the Q value of each state (the value of each state-action combination) using first visit Monte Carlo. q_value_array holds the estimated values. After each step the agent takes in the env, we update the q_value_array with the first visit Monte Carlo pseudocode seen in the video. Pseudocode is from http://incompleteideas.net/book/the-book.html. \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1PxGyfu124QLrSL77NwDEdRUlJZE2ehh2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHliEwLn8sf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def monte_carlo_first_visit_update(q_values, q_returns, traj, discount=1.):\n",
        "  g_return = 0.\n",
        "  # dictionary tracking first visit\n",
        "  first_visit_dict = {}\n",
        "  # iterate through trajectory\n",
        "  for t in range(len(traj)-1,-1,-1):\n",
        "    state, reward, action = traj[t]\n",
        "    # calculate return\n",
        "    g_return = discount*g_return + reward\n",
        "    if (state, action) not in first_visit_dict:\n",
        "      first_visit_dict[(state,action)] = 1\n",
        "      # calculate average return. we do a running average\n",
        "      q_returns[state][action][1] += 1 # counter of how many returns for this state and action\n",
        "      q_returns[state][action][0] = (q_returns[state][action][0] * (q_returns[state][action][1]-1) + g_return)/ q_returns[state][action][1]\n",
        "      # update the q_value with average return\n",
        "      q_values[state][action] = q_returns[state][action][0]\n",
        "      # in pseudocode you find argmax action here too; in this code we do it at action selection time\n",
        "  \n",
        "  return q_values, q_returns\n",
        "\n",
        "def monte_carlo_q_value_estimate(env,episodes=1000,discount_factor=1.0,epsilon=0.1):\n",
        "  state_size = env.nS\n",
        "  action_size = env.nA\n",
        "  max_timesteps = 100 # halt episode after this many timesteps\n",
        "  timesteps = 0\n",
        "  #initialize the estimated state values to zero\n",
        "  q_value_array = np.zeros((state_size, action_size))\n",
        "  #initialize the collected array for holding returns; we use a running average\n",
        "  q_return_array = np.zeros((state_size,action_size,2))\n",
        "  \n",
        "  #list for holding trajectories\n",
        "  trajectory_list = []\n",
        "  \n",
        "  #reset the env\n",
        "  current_state = env.reset()\n",
        "  #env._render()\n",
        "\n",
        "  #run through each episode taking a random action each time\n",
        "  #upgrade estimated state value after each action\n",
        "  current_episode = 0\n",
        "  while current_episode < episodes:\n",
        "    #choose action based on epsilon-greedy policy\n",
        "    if np.random.rand() < epsilon:\n",
        "      eg_action = env.action_space.sample()\n",
        "    else:\n",
        "      #Choose a greedy action from available max actions\n",
        "      argmax_index = np.argmax(q_value_array[current_state])\n",
        "      argmax_value = q_value_array[current_state][argmax_index]\n",
        "      greedy_indices = np.argwhere(q_value_array[current_state] == argmax_value).reshape(-1)\n",
        "      eg_action = np.random.choice(greedy_indices)\n",
        "\n",
        "    #take a step using epsilon-greedy action\n",
        "    next_state, rew, done, info = env.step(eg_action)\n",
        "    trajectory_list.append((current_state,rew,eg_action))\n",
        "    #optional: end gridworld early if too many timesteps taken in an episode\n",
        "    timesteps += 1\n",
        "    if timesteps > max_timesteps:\n",
        "      done = 1\n",
        "\n",
        "    #if the episode is done use Monte Carlo to update q values and reset the env\n",
        "    if done:\n",
        "      q_value_array, q_return_array = monte_carlo_first_visit_update(q_value_array, q_return_array, trajectory_list, discount_factor)\n",
        "      trajectory_list = []\n",
        "      timesteps = 0\n",
        "      current_state = env.reset()\n",
        "      current_episode += 1\n",
        "    else:\n",
        "      current_state = next_state\n",
        "\n",
        "  return q_value_array, q_return_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDcyg7ITu7sx",
        "colab_type": "code",
        "outputId": "2c74be66-c046-4938-c284-6bba6e003032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "source": [
        "env = GridworldEnv()\n",
        "\n",
        "#run episodes with Monte Carlo method and get the Q value estimates\n",
        "q_values, q_returns = monte_carlo_q_value_estimate(env, episodes=10000, discount_factor=1., epsilon=0.1)\n",
        "\n",
        "print(\"All Q Value Estimates:\")\n",
        "print(np.round(q_values.reshape((16,4)),1))\n",
        "print(\"each row is a state, each column is an action\")\n",
        "print(\"\")\n",
        "\n",
        "#action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n",
        "greedy_q_value_estimates = np.max(q_values,axis=1)\n",
        "print(\"Greedy Q Value Estimates:\")\n",
        "print(np.round(greedy_q_value_estimates.reshape(env.shape),1))\n",
        "print(\"estimate of the optimal state value at each state\")\n",
        "print(\"\")\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Q Value Estimates:\n",
            "[[  0.    0.    0.    0. ]\n",
            " [ -2.1  -3.9  -4.   -1. ]\n",
            " [ -5.  -11.3  -5.9  -2.1]\n",
            " [-16.9 -14.   -3.4  -5.8]\n",
            " [ -1.   -5.8  -5.6  -4.3]\n",
            " [ -2.1  -7.4  -4.3  -3.1]\n",
            " [ -6.7 -14.6  -5.2  -3.3]\n",
            " [ -4.7 -15.1  -2.2  -6.6]\n",
            " [ -2.1  -4.5  -5.4  -4.7]\n",
            " [ -3.2  -4.   -4.4  -6.2]\n",
            " [ -5.1  -4.4  -2.1  -5.8]\n",
            " [ -5.5  -2.2  -1.   -4.6]\n",
            " [ -5.6  -3.2  -6.4  -5.3]\n",
            " [ -7.8  -2.1  -3.5  -6. ]\n",
            " [ -3.4  -1.   -2.2  -3.4]\n",
            " [  0.    0.    0.    0. ]]\n",
            "each row is a state, each column is an action\n",
            "\n",
            "Greedy Q Value Estimates:\n",
            "[[ 0.  -1.  -2.1 -3.4]\n",
            " [-1.  -2.1 -3.3 -2.2]\n",
            " [-2.1 -3.2 -2.1 -1. ]\n",
            " [-3.2 -2.1 -1.   0. ]]\n",
            "estimate of the optimal state value at each state\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QldcKOOtt75a",
        "colab_type": "text"
      },
      "source": [
        "The first output shows the estimated value for each action in each state. Ie row 4 column 4 is the value if the agent was in the upper right grid cell and took that action left. In the second output, we take the best action for each of the 16 states and show the agent's estimate of the state value assuming the agent always acts greedily."
      ]
    }
  ]
}
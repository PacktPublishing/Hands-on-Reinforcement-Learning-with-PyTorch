{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.2 SARSA & SARSE.ipynb","version":"0.3.2","provenance":[{"file_id":"1J4zEuSgvtZvG6hQJCylcH3XLODZceU2f","timestamp":1549649307856},{"file_id":"1oW2DyHFO8smkVrlT6zt8dgEmdnDflnAe","timestamp":1549649256657},{"file_id":"1ru_y_J0AWJjsVR4tL1NpcJYBqo04AsQr","timestamp":1549644496478}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"npzJ574a6A94","colab_type":"text"},"source":["**Reinforcement Learning with SARSE & SARSE**\n","* This notebook shows how to apply the classic Reinforcement Learning (RL) concepts of SARSA and SARSE\n","* In SARSA, we estimate Q values: Q(s,a) like we do in Q learning. However in SARSA we do on-policy updates while in Q learning we do off-policy updates\n","* We can create a policy from the Q values. Two types of policy categorizations are on-policy and off-policy methods. \n","* In off-policy methods we use one policy for exploration (behavior policy) while we learn a separate policy (target policy). In on-policy methods, the exploration and learned policy are the same. In SARSA we explore with the policy we are learning.\n","* SARSE is a slight variation of SARSA. In SARSA the next state is found by sampling an action from the policy, in SARSE the next state is the expected value of all states weighted by the policy. In SARS**A** we take an **A**ction while in SARS**E** we use **E**xpected value.\n","\n","Outline:\n","1. Define the GridWorld environment\n","2. Discuss On-policy and Off-policy methods\n","3. Find the Q values in the environment using SARSA\n","4. Find the Q values in the environment using SARSE\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B-RZQ2KOmG9D","colab_type":"text"},"source":["**GridWorld**\n","\n","The GridWorld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right."]},{"cell_type":"code","metadata":{"id":"0v8rA8v67PKc","colab_type":"code","colab":{}},"source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","#https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/DP/Value%20Iteration%20Solution.ipynb\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKeUO0LKnCgp","colab_type":"text"},"source":["**Policies: On-Policy vs. Off-Policy**\n","\n","A policy is the agent's action selection method for each state (a probability distribution over actions). This can be a deterministic choice like a greedy policy where the highest valued action is always chosen or a stochastic choice like in the TD learning notebook were we used a random policy at each state. Two categorizations of policies are on-policy and off-policy methods. SARSA and Q learning are very similar. The difference is in how the  Q value estimate is updated. In Q learning the update is off-policy, in SARSA the update is on-policy.\n","\n","In off-policy methods we use one policy for exploration (behavior policy) while we learn a separate policy (target policy). In on-policy methods, the exploration and learned policy are the same. In SARSA we explore and learn with one policy. The difference is in how we use the TD error. In Q learning the TD error is:\n","\n","reward + gamma*max(Q(s',a)) - current_state_estimate. \n","\n","The max value isn't based on the current policy that the agent is actually following, it's based on a greedy policy that is always selecting the highest Q value estimate. Contrast this to SARSA where the TD error is:\n","\n","reward + gamma*Q(s',sampled_action) - current_state_estimate\n","\n","In SARSA we sample the next action selected from the policy and use that for our next Q value estimate. The code cell below has the updates side by side. SARSA is making updates using the policy that SARSA is exploring the env with.\n"]},{"cell_type":"code","metadata":{"id":"YoXR4S1JpOuN","colab_type":"code","outputId":"3e9c96ef-36d9-4d5f-b32b-f7774f2fc184","executionInfo":{"status":"ok","timestamp":1558655273399,"user_tz":240,"elapsed":285,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1508}},"source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","current_state = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","q_table = np.array([[ 0.,   0.,   0.,   0. ],\n"," [-1.7, -2.4, -2.2, -1. ],\n"," [-2.3, -2.8, -2.6, -2. ],\n"," [-3.2, -3.3, -3.,  -3. ],\n"," [-1.,  -2.4, -2.6, -1.8],\n"," [-2.,  -2.8, -2.5, -2. ],\n"," [-3.,  -3.,  -3.,  -3. ],\n"," [-2.7, -2.5, -2.,  -2.5],\n"," [-2.,  -2.4, -2.6, -2.4],\n"," [-3.,  -3.,  -3.,  -3. ],\n"," [-2.5, -2.,  -2.,  -2.9],\n"," [-1.9, -1.5, -1.,  -2.3],\n"," [-3.,  -3.,  -3.5, -3.1],\n"," [-2.9, -2.,  -2.6, -2.9],\n"," [-2.5, -1.,  -1.6, -2.3],\n"," [ 0.,   0.,   0.,   0. ]])\n","alpha = 0.1\n","gamma = 1.\n","\n","epsilon = 0.1\n","\n","def get_action(s):\n","  #choose random action epsilon amount of the time\n","  if np.random.rand() < epsilon:\n","    action = env.action_space.sample()\n","    action_type = \"random\"\n","  else:\n","    #Choose a greedy action.\n","    action = np.argmax(q_table[s])\n","    action_type = \"greedy\"\n","  return action, action_type\n","   \n","action,action_type = get_action(current_state)\n","\n","for i in range(10):\n","  next_state,reward,done,info = env.step(action)\n","  print(\"Agent took {} action {} and is now in state {} \".format(action_type, action_dict[action], current_state))\n","  #in SARSA we find our next action based on the current policy (on-policy). In Q learning we don't need the next action, we take the max of the next state\n","  next_action, action_type = get_action(next_state) \n","  \n","  #update q table on-policy (SARSA)\n","  q_table[current_state,action] = q_table[current_state,action] + alpha*(gamma*q_table[next_state,next_action] - q_table[current_state,action])\n","  \n","  #For reference update q table off-policy (Q learning)\n","  #q_table[current_state,action] = q_table[current_state,action] + alpha*(gamma*np.max(q_table[next_state]) - q_table[current_state,action])\n","  \n","  env._render()\n","  print(\"\")\n","  if done:\n","    print(\"Agent reached end of episode, resetting the env\")\n","    current_state = env.reset()\n","    print(\"\")\n","    env._render()\n","    print(\"\")\n","  else:\n","    current_state = next_state\n","    action = next_action"],"execution_count":2,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action UP and is now in state 4 \n","x  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action UP and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  x  o\n","o  o  o  T\n","\n","Agent took greedy action RIGHT and is now in state 10 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  x\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 11 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 2 \n","T  o  o  o\n","o  o  x  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action UP and is now in state 6 \n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 2 \n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 1 \n","x  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  o\n","o  x  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 9 \n","T  o  o  o\n","o  o  o  o\n","x  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action UP and is now in state 8 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g4Ge-npOyKt0","colab_type":"text"},"source":["**The RL Training Loop**\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the Q value of each state (the value of each state-action combination) using SARSA. q_value_array holds the estimated values. After each step the agent takes in the env, we update the q_value_array with the SARSA formula. The SARSA loop differs in that prior to updating the estimate, we select the next action. We use the next action in the update and then in the agent's next step we use that next action as the action to take."]},{"cell_type":"code","metadata":{"id":"rHliEwLn8sf-","colab_type":"code","colab":{}},"source":["def choose_action(q_table, state, epsilon=0.1):\n","  #choose action based on epsilon-greedy policy\n","  if np.random.rand() < epsilon:\n","    eg_action = env.action_space.sample()\n","  else:\n","    #Choose a greedy action\n","    eg_action = np.argmax(q_table[state])\n","  return eg_action\n","\n","def sarsa_q_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  state_size = env.nS\n","  action_size = env.nA\n","  #initialize the estimated state values to zero\n","  q_value_array = np.zeros((state_size, action_size))\n","  #reset the env\n","  current_state = env.reset()\n","  eg_action = choose_action(q_value_array, current_state, epsilon)\n","\n","  #run through each episode taking a random action each time\n","  #upgrade estimated state value after each action\n","  current_episode = 0\n","  while current_episode < episodes:\n","\n","    #take a step using epsilon-greedy action\n","    next_state, rew, done, info = env.step(eg_action)\n","    next_action = choose_action(q_value_array, next_state, epsilon)\n","\n","    #update Q-values using SARSA\n","    q_value_array[current_state,eg_action] = q_value_array[current_state,eg_action] + \\\n","       alpha * (rew + discount_factor*q_value_array[next_state,next_action] - q_value_array[current_state,eg_action])\n","\n","    #if the episode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","    if done:\n","      current_state = env.reset()\n","      eg_action = choose_action(q_value_array, current_state, epsilon)\n","      current_episode += 1\n","    else:\n","      current_state = next_state\n","      eg_action = next_action\n","\n","  return q_value_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDcyg7ITu7sx","colab_type":"code","outputId":"27ae9682-b245-4a59-c7d5-d5d482fdd36e","executionInfo":{"status":"ok","timestamp":1558655705147,"user_tz":240,"elapsed":795,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":468}},"source":["#run episodes with SARSA and get the state value estimates\n","q_values = sarsa_q_value_estimate(env,episodes=10000,alpha=0.01)\n","\n","print(\"All Q Value Estimates:\")\n","print(np.round(q_values.reshape((16,4)),2))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","greedy_q_value_estimates = np.max(q_values,axis=1)\n","print(\"Greedy Policy Q Value Estimates:\")\n","print(np.round(greedy_q_value_estimates.reshape(env.shape),2))\n","print(\"estimate of the current state value at each state using a greedy policy\")\n","print(\"\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["All Q Value Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [-1.38 -1.82 -1.67 -1.  ]\n"," [-2.16 -2.28 -2.25 -2.03]\n"," [-2.99 -2.98 -2.97 -2.97]\n"," [-1.   -1.75 -1.83 -1.47]\n"," [-2.03 -2.18 -2.12 -2.04]\n"," [-2.77 -2.77 -2.77 -2.77]\n"," [-2.3  -2.19 -2.04 -2.36]\n"," [-2.04 -2.34 -2.36 -2.17]\n"," [-2.77 -2.78 -2.78 -2.78]\n"," [-2.16 -2.03 -2.02 -2.18]\n"," [-1.77 -1.39 -1.   -1.67]\n"," [-2.96 -2.96 -2.98 -2.98]\n"," [-2.25 -2.04 -2.1  -2.28]\n"," [-1.58 -1.   -1.4  -1.84]\n"," [ 0.    0.    0.    0.  ]]\n","each row is a state, each column is an action\n","\n","Greedy Policy Q Value Estimates:\n","[[ 0.   -1.   -2.03 -2.97]\n"," [-1.   -2.03 -2.77 -2.04]\n"," [-2.04 -2.77 -2.02 -1.  ]\n"," [-2.96 -2.04 -1.    0.  ]]\n","estimate of the current state value at each state using a greedy policy\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gy6vmJ8nnamW","colab_type":"text"},"source":["**SARSE vs. SARSA**\n","\n","SARSE slightly modifies SARSA. While in SARSA we sample to get the next action, in SARSE we use the policy probabilities to create an expected value of the next state estimate. For example, with SARSA we used epsilon-greedy exploration to get the next action. 92.5% of the time SARSA chose the greedy action (90% greedy + 2.5% random) and 2.5% of the time each of the other non-greedy actions were chosen. SARSE uses these probabilities (0.925, 0.025, 0.025, 0.025) and the Q value estimates to create an expectation. The TD error update becomes:\n","\n","reward + gamma*next_state_estimate - current_state_estimate\n","\n","\n","where next_state_estimate is:\n","\n","next_state_estimate = 0.925 x q_table[next_state_0,next_action_0] + 0.025 x q_table[next_state_1,next_action_1]  + 0.025 x q_table[next_state_2, next_action_2] + 0.025 x q_table[next_state_3,next_action_3]\n","\n","SARSE is on-policy."]},{"cell_type":"code","metadata":{"id":"KyNqdl1rqoP7","colab_type":"code","colab":{}},"source":["def sarse_q_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  state_size = env.nS\n","  action_size = env.nA\n","  #initialize the estimated state values to zero\n","  q_value_array = np.zeros((state_size, action_size))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  #chance of choosing random action based on epsilon. use this with SARSE's action probabilities\n","  random_prob = epsilon/action_size\n","  greedy_prob = 1.-epsilon\n","\n","  #run through each episode taking a random action each time\n","  #upgrade estimated state value after each action\n","  current_episode = 0\n","  while current_episode < episodes:\n","    #choose action based on epsilon-greedy policy\n","    if np.random.rand() < epsilon:\n","      eg_action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","      eg_action = np.argmax(q_value_array[current_state])\n","\n","    #take a step using epsilon-greedy action\n","    next_state, rew, done, info = env.step(eg_action)\n","\n","    #generate action probabilities\n","    #randomly choose each action with probability epislon/4 \n","    action_probs = np.array([random_prob]*action_size) \n","    #choose greedy action with probability 1-epsilon\n","    action_probs[np.argmax(q_value_array[next_state])] += greedy_prob \n","\n","    #Update Q-values with SARSE\n","    next_action_value_estimate = 0.\n","    for i in range(action_size):\n","      next_action_value_estimate += action_probs[i] * q_value_array[next_state,i]\n","    q_value_array[current_state,eg_action] = q_value_array[current_state,eg_action] + \\\n","       alpha * (rew + discount_factor*next_action_value_estimate - q_value_array[current_state,eg_action])\n","\n","    #if the episode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","    if done:\n","      current_state = env.reset()\n","      current_episode += 1\n","    else:\n","      current_state = next_state\n","\n","  return q_value_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2qwyyRAqrJ2","colab_type":"code","outputId":"3b98887d-70e2-4e93-bce3-db2f4b75aab7","executionInfo":{"status":"ok","timestamp":1558656104477,"user_tz":240,"elapsed":849,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":468}},"source":["#run episodes with SARSE and get the state value estimates\n","q_values = sarse_q_value_estimate(env,episodes=10000,alpha=0.01)\n","\n","print(\"All Q Values Estimates:\")\n","print(np.round(q_values.reshape((16,4)),2))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","greedy_q_value_estimates = np.max(q_values,axis=1)\n","print(\"Greedy Policy State Value Estimates:\")\n","print(np.round(greedy_q_value_estimates.reshape(env.shape),2))\n","print(\"estimate of the current state value at each state under a greedy policy\")\n","print(\"\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["All Q Values Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [-1.32 -1.89 -1.73 -1.  ]\n"," [-2.25 -2.35 -2.4  -2.04]\n"," [-3.   -2.99 -2.98 -2.98]\n"," [-1.   -1.76 -1.58 -1.4 ]\n"," [-2.03 -2.15 -2.22 -2.03]\n"," [-2.79 -2.79 -2.79 -2.79]\n"," [-2.26 -2.2  -2.05 -2.36]\n"," [-2.04 -2.3  -2.31 -2.14]\n"," [-2.78 -2.78 -2.78 -2.78]\n"," [-2.24 -2.03 -2.03 -2.2 ]\n"," [-1.91 -1.37 -1.   -1.73]\n"," [-2.96 -2.96 -2.97 -2.97]\n"," [-2.29 -2.05 -2.16 -2.3 ]\n"," [-1.74 -1.   -1.41 -1.83]\n"," [ 0.    0.    0.    0.  ]]\n","each row is a state, each column is an action\n","\n","Greedy Policy State Value Estimates:\n","[[ 0.   -1.   -2.04 -2.98]\n"," [-1.   -2.03 -2.79 -2.05]\n"," [-2.04 -2.78 -2.03 -1.  ]\n"," [-2.96 -2.05 -1.    0.  ]]\n","estimate of the current state value at each state under a greedy policy\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VO3mJzJ0r6ki","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
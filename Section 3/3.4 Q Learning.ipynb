{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.3 Q Learning.ipynb","version":"0.3.2","provenance":[{"file_id":"1ru_y_J0AWJjsVR4tL1NpcJYBqo04AsQr","timestamp":1549644496478}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"npzJ574a6A94","colab_type":"text"},"source":["**Reinforcement Learning with Q Learning**\n","* This notebook shows how to apply the classic Reinforcement Learning (RL) idea of Q learning \n","* In TD learning we estimated state values: V(s). In Q learning we estimate Q values: Q(s,a). Here we'll go over Q learning in the simple tabular case.\n","* A key concept in RL is exploration. We'll use epsilon greedy exploration, which is often used with Q learning.\n","\n","Outline:\n","1. Define the GridWorld environment\n","2. Discuss Epsilon-Greedy Exploration\n","3. Find the value of each Q value in the environment using Q learning\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B-RZQ2KOmG9D","colab_type":"text"},"source":["**GridWorld**\n","\n","The GridWorld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right."]},{"cell_type":"code","metadata":{"id":"0v8rA8v67PKc","colab_type":"code","colab":{}},"source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKeUO0LKnCgp","colab_type":"text"},"source":["**An Introduction to Exploration: Epsilon-Greedy Exploration**\n","\n","Exploration is a key concept in RL. In order to find the best policies, an agent needs to explore the environment. By exploring, the agent can experience new states and rewards. In the TD learning notebook, the agent explored GridWorld by taking a random action at every step. While random action explorations can work in some environments, the downside is the agent can spend too much time exploring bad states or states that have already been explored fully and not enough time exploring promising states. A simple--yet surprisingly effective--approach to exploration is Epsilon-Greedy exploration. A epsilon percentage of the time, the agent chooses a random action. The remaining amount of the time (1-epsilon) the agent choose the best estimated action aka the *greedy action*. Epsilon can be a fixed value between 0 and 1 or can start at a high value and gradually decay over time (ie start at .99 and decay to 0.01). In this notebook we will used a fixed epsilon value of 0.1. Below is a simple example of epsilon-greedy exploration.\n"]},{"cell_type":"code","metadata":{"id":"YoXR4S1JpOuN","colab_type":"code","outputId":"b422eac3-3fbc-4652-be0a-82a9f3f567a3","executionInfo":{"status":"ok","timestamp":1558656612482,"user_tz":240,"elapsed":289,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1560}},"source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","current_state = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","greedy_dict = {0:3,1:3,2:3,3:3,\n","              4:0,5:0,6:0,7:0,\n","              8:2,9:2,10:2,11:2,\n","              12:1,13:1,14:1,15:1}\n","epsilon = 0.1\n","\n","for i in range(10):\n","  #choose random action epsilon amount of the time\n","  if np.random.rand() < epsilon:\n","    action = env.action_space.sample()\n","    action_type = \"random\"\n","  else:\n","    #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","    action = greedy_dict[current_state]\n","    action_type = \"greedy\"\n"," \n","  current_state,reward,done,info = env.step(action)\n","  print(\"Agent took {} action {} and is now in state {} \".format(action_type, action_dict[action], current_state))\n","  env._render()\n","  print(\"\")\n","  if done:\n","    print(\"Agent reached end of episode, resetting the env\")\n","    print(env.reset())\n","    print(\"\")\n","    env._render()\n","    print(\"\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","o  o  x  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action UP and is now in state 2 \n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 1 \n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 0 \n","x  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent reached end of episode, resetting the env\n","1\n","\n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 0 \n","x  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent reached end of episode, resetting the env\n","10\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  x  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 9 \n","T  o  o  o\n","o  o  o  o\n","o  x  o  o\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 13 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n","Agent took greedy action RIGHT and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action RIGHT and is now in state 15 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","8\n","\n","T  o  o  o\n","o  o  o  o\n","x  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action RIGHT and is now in state 9 \n","T  o  o  o\n","o  o  o  o\n","o  x  o  o\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 13 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"azurqbzns_EW","colab_type":"text"},"source":["**The RL Training Loop**\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the Q value of each state (the value of each state-action combination) using Q learning. q_value_array holds the estimated values. After each step the agent takes in the env, we update the q_value_array with the Q learning formula."]},{"cell_type":"code","metadata":{"id":"rHliEwLn8sf-","colab_type":"code","colab":{}},"source":["def q_learning_q_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  state_size = env.nS\n","  action_size = env.nA\n","  #initialize the estimated state values to zero\n","  q_value_array = np.zeros((state_size, action_size))\n","  #reset the env\n","  current_state = env.reset()\n","  #env._render()\n","\n","  #run through each episode taking a random action each time\n","  #upgrade estimated state value after each action\n","  current_episode = 0\n","  while current_episode < episodes:\n","    #choose action based on epsilon-greedy policy\n","    if np.random.rand() < epsilon:\n","      eg_action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action.\n","      eg_action = np.argmax(q_value_array[current_state])\n","\n","    #take a step using epsilon-greedy action\n","    next_state, rew, done, info = env.step(eg_action)\n","\n","    #Update Q values using Q learning update method\n","    max_q_value = np.max(q_value_array[next_state])\n","    q_value_array[current_state,eg_action] = q_value_array[current_state,eg_action] + \\\n","      alpha * (rew + discount_factor*max_q_value - q_value_array[current_state,eg_action])\n","\n","    #if the epsiode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","    if done:\n","      current_state = env.reset()\n","      current_episode += 1\n","    else:\n","      current_state = next_state\n","\n","  return q_value_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDcyg7ITu7sx","colab_type":"code","outputId":"22351ac5-f4e7-48fc-b3ca-444e982c4dcb","executionInfo":{"status":"ok","timestamp":1558656636531,"user_tz":240,"elapsed":898,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":468}},"source":["#run episodes with Q learning and get the state value estimates\n","q_values = q_learning_q_value_estimate(env,episodes=10000,alpha=0.01)\n","\n","print(\"All Q Value Estimates:\")\n","print(np.round(q_values.reshape((16,4)),1))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","greedy_q_value_estimates = np.max(q_values,axis=1)\n","print(\"Greedy Q Value Estimates:\")\n","print(np.round(greedy_q_value_estimates.reshape(env.shape),1))\n","print(\"estimate of the optimal State value at each state\")\n","print(\"\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["All Q Value Estimates:\n","[[ 0.   0.   0.   0. ]\n"," [-1.4 -1.8 -1.8 -1. ]\n"," [-2.1 -2.4 -2.2 -2. ]\n"," [-3.  -2.9 -2.9 -2.9]\n"," [-1.  -1.6 -1.7 -1.4]\n"," [-2.  -2.2 -2.2 -2. ]\n"," [-2.7 -2.7 -2.7 -2.7]\n"," [-2.3 -2.2 -2.  -2.2]\n"," [-2.  -2.2 -2.3 -2.2]\n"," [-2.7 -2.7 -2.7 -2.7]\n"," [-2.2 -2.  -2.  -2.1]\n"," [-1.8 -1.4 -1.  -1.8]\n"," [-2.9 -2.9 -2.9 -3. ]\n"," [-2.2 -2.  -2.1 -2.4]\n"," [-1.7 -1.  -1.4 -1.9]\n"," [ 0.   0.   0.   0. ]]\n","each row is a state, each column is an action\n","\n","Greedy Q Value Estimates:\n","[[ 0.  -1.  -2.  -2.9]\n"," [-1.  -2.  -2.7 -2. ]\n"," [-2.  -2.7 -2.  -1. ]\n"," [-2.9 -2.  -1.   0. ]]\n","estimate of the optimal State value at each state\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QldcKOOtt75a","colab_type":"text"},"source":["The first output shows the estimated value for each action in each state. Ie row 4 column 4 is the value if the agent was in the upper right grid cell and took that action left. In the second output, we take the best action for each of the 16 states and show the agent's estimate of the state value assuming the agent always acts greedily."]},{"cell_type":"code","metadata":{"id":"Yd_0h1EgN-W4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
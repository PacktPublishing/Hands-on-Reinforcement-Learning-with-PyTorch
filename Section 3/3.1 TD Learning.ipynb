{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.1 TD Learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"npzJ574a6A94"},"source":["**Reinforcement Learning with TD Learning**\n","\n","Outline:\n","1. Define the GridWorld environment\n","2. Find the value of each state value in the environment using TD learning (TD(0)) and a random policy\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B-RZQ2KOmG9D"},"source":["**Gridworld**\n","\n","The Gridworld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right. Gridworld code from Denny Britz's Reinforcement Learning repo: https://github.com/dennybritz/reinforcement-learning\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0v8rA8v67PKc","colab":{}},"source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558654818065,"user_tz":240,"elapsed":265,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"id":"YoXR4S1JpOuN","outputId":"b1028c7d-f1bb-4f9b-fcd8-77559ddd9a3e","colab":{"base_uri":"https://localhost:8080/","height":1144}},"source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","observation = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","\n","for i in range(10):\n","    #get a random action\n","    random_action = env.action_space.sample()\n","    observation,reward,done,info = env.step(random_action)\n","    print(\"Agent took action {} and is now in state {} \".format(action_dict[random_action], observation))\n","    env._render()\n","    print(\"\")\n","    if done:\n","        print(\"Agent reached end of episode, resetting the env\")\n","        print(env.reset())\n","        print(\"\")\n","        env._render()\n","        print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","x  o  o  T\n","\n","Agent took action UP and is now in state 8 \n","T  o  o  o\n","o  o  o  o\n","x  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 8 \n","T  o  o  o\n","o  o  o  o\n","x  o  o  o\n","o  o  o  T\n","\n","Agent took action RIGHT and is now in state 9 \n","T  o  o  o\n","o  o  o  o\n","o  x  o  o\n","o  o  o  T\n","\n","Agent took action UP and is now in state 5 \n","T  o  o  o\n","o  x  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action RIGHT and is now in state 5 \n","T  o  o  o\n","o  x  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GpiR5UEXpSnN"},"source":["**The RL Training Loop**\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the value of each state (each cell in the gridworld) under a random policy using TD(0). state_value_array holds the estimated values and after each step the agent takes in the env, we update the state_value_array with the TD(0) formula.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rHliEwLn8sf-","colab":{}},"source":["def td_learning_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0):\n","  #initialize the estimated state values to zero\n","  state_size = env.nS\n","  state_value_array = np.zeros(state_size)\n","  #reset the env\n","  current_state = env.reset()\n","  #env._render()\n","\n","  #run through each episode taking a random action each time\n","  #update estimated state value after each action\n","  current_episode = 0\n","  while current_episode < episodes:\n","    #take a random action\n","    random_action = env.action_space.sample()\n","    next_state, rew, done, info = env.step(random_action)\n","\n","    #update state values using TD(0)\n","    state_value_array[current_state] = state_value_array[current_state] + \\\n","      alpha * (rew + discount_factor*state_value_array[next_state] -state_value_array[current_state])\n","\n","    #if episode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","    if done:\n","      current_state = env.reset()\n","      current_episode += 1\n","    else:\n","      current_state = next_state\n","\n","  return state_value_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558655351284,"user_tz":240,"elapsed":2000,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"id":"eDcyg7ITu7sx","outputId":"5554ba81-b2cc-413b-e401-1e3e112dfae2","colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["#run episodes with TD learning and get the state value estimates\n","state_values = td_learning_value_estimate(env,episodes=10000,alpha=0.01)\n","\n","print(\"State Value Estimates:\")\n","print(np.round(state_values,2))\n","print(\"\")\n","\n","print(\"Reshaped State Value Estimates:\")\n","print(np.round(state_values.reshape(env.shape),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["State Value Estimates:\n","[  0.   -14.12 -19.95 -22.11 -14.39 -18.12 -20.1  -20.33 -20.52 -20.17\n"," -18.05 -14.43 -22.27 -20.02 -13.36   0.  ]\n","\n","Reshaped State Value Estimates:\n","[[  0.   -14.12 -19.95 -22.11]\n"," [-14.39 -18.12 -20.1  -20.33]\n"," [-20.52 -20.17 -18.05 -14.43]\n"," [-22.27 -20.02 -13.36   0.  ]]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gg90mb4sqTQy"},"source":["The 'Reshaped State Value Estimates' show the TD learning estimate for the state values. The closer the agent is to a terminal state, the higher the estimate (since the agent is more likely to randomly choose an action and end up ending the episode)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3PoyqsvCFDkQ","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
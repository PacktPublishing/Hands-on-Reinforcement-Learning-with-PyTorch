{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (dense_layer_1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (dense_layer_2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create simple Neural Network\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # create fully connected layers\n",
    "        self.dense_layer_1 = nn.Linear(input_size, hidden_size) #y = xA^T+b \n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # create output\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # use forward to construct the forward pass of the computational graph\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "my_neural_network = NeuralNetwork(input_size=5, output_size=1, hidden_size=32)\n",
    "print(my_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (dense_layer_1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (dense_layer_2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#different activation functions\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # create fully connected layers\n",
    "        self.dense_layer_1 = nn.Linear(input_size, hidden_size) #y = xA^T+b \n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # create output\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # use forward to construct the forward pass of the computational graph\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return torch.tanh(self.output(x)) #squash output between -1 and 1\n",
    "\n",
    "my_neural_network = NeuralNetwork(input_size=5, output_size=1, hidden_size=32)\n",
    "print(my_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNeuralNetwork(\n",
      "  (dense_layer_1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense_layer_2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Feed Forward Neural Network with Batch Normalization\n",
    "\n",
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        # create fully connected layers\n",
    "        self.dense_layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(hidden_size)\n",
    "        # create output\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # use forward to construct the forward pass of the computational graph\n",
    "        x = F.relu(self.batch_norm_1(self.dense_layer_1(x)))\n",
    "        x = F.relu(self.batch_norm_2(self.dense_layer_2(x)))\n",
    "        return self.output(x)\n",
    "\n",
    "my_neural_network = FeedForwardNeuralNetwork(input_size=5, output_size=1, hidden_size=32)\n",
    "print(my_neural_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQNetwork(\n",
      "  (conv_layer_1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv_layer_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (conv_layer_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc_layer): Linear(in_features=3136, out_features=32, bias=True)\n",
      "  (out_layer): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, num_action, hidden_size):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv_layer_1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv_layer_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_layer = nn.Linear(7 * 7 * 64, hidden_size)\n",
    "        self.out_layer = nn.Linear(hidden_size, num_action)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_layer_1(x))\n",
    "        x = F.relu(self.conv_layer_2(x))\n",
    "        x = F.relu(self.conv_layer_3(x))\n",
    "        x = F.relu(self.fc_layer(x.view(x.size(0), -1)))\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "    \n",
    "my_dqn = DeepQNetwork(num_action=6, hidden_size=32)\n",
    "print(my_dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_pytorch",
   "language": "python",
   "name": "rl_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

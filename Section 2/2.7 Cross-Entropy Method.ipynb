{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create deterministic version of Frozen Lake\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sample(env, policy, action_size):\n",
    "    state = env.reset()\n",
    "    # get action counts for each sample\n",
    "    action_count = np.zeros(np.shape(policy))\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = np.random.choice(action_size,p=policy[state])\n",
    "        action_count[state,action] += 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, action_count\n",
    "\n",
    "def update_policy(policy, s_list, learning_rate, keep):\n",
    "    # only keep best samples\n",
    "    s_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    s_list = s_list[:keep]\n",
    "    \n",
    "    # get the action counts for the best performers in one array\n",
    "    best_policy = np.zeros(np.shape(policy))\n",
    "    for s in s_list:\n",
    "        best_policy += s[1]\n",
    "       \n",
    "    # update policy using learning rate\n",
    "    for i in range(len(best_policy)):\n",
    "        total_actions = np.sum(best_policy[i])\n",
    "        if total_actions > 0:\n",
    "            policy[i] = (best_policy[i]/total_actions)*learning_rate + policy[i]*(1.-learning_rate) \n",
    "    \n",
    "    # normalize the probabilities of the policy so that they sum to 1\n",
    "    policy = policy / np.sum(policy,axis=1)[:,None]\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake Not Slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "trial = 100\n",
    "keep_best = int(0.2*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize policy\n",
    "policy_array = np.ones((state_size, action_size))/action_size\n",
    "\n",
    "# run trials and collect samples with each trial\n",
    "# update policy at end of each trial\n",
    "for t in range(trial):\n",
    "    sample_list = []\n",
    "    for s in range(samples):\n",
    "        reward, action_table = run_sample(env, policy_array, action_size)\n",
    "        sample_list.append((reward, action_table))\n",
    "    policy_array = update_policy(policy_array, sample_list, learning_rate, keep_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 1.0 Average Length: 6.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        action = np.argmax(policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake Slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "trial = 1000\n",
    "keep_best = int(0.2*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policy\n",
    "policy_array = np.ones((state_size, action_size))/action_size\n",
    "\n",
    "# run trials and collect samples with each trial\n",
    "# update policy at end of each trial\n",
    "for t in range(trial):\n",
    "    sample_list = []\n",
    "    for s in range(samples):\n",
    "        reward, action_table = run_sample(env, policy_array, action_size)\n",
    "        sample_list.append((reward, action_table))\n",
    "    policy_array = update_policy(policy_array, sample_list, learning_rate, keep_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 0.73 Average Length: 40.6\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        action = np.argmax(policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('Taxi-v2')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "trial = 1000\n",
    "keep_best = int(0.5*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_array = np.ones((state_size, action_size))/action_size\n",
    "\n",
    "for t in range(trial):\n",
    "    sample_list = []\n",
    "    for s in range(samples):\n",
    "        reward, action_table = run_sample(env, policy_array, action_size)\n",
    "        sample_list.append((reward, action_table))\n",
    "    policy_array = update_policy(policy_array, sample_list, learning_rate, keep_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 8.32 Average Length: 12.68\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        # choose action based on best action in that state\n",
    "        action = np.argmax(policy_array[state])\n",
    "        # choose action based policy distribution\n",
    "        #action = np.random.choice(action_size,p=policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross-Entropy Method using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda') \n",
    "else:                                                   \n",
    "    device = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "smoothing_factor = 1\n",
    "trial = 200\n",
    "keep_best = int(0.2*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sample_tensor(env, policy, state_size, action_size, device):\n",
    "    state = env.reset()\n",
    "    # get action counts for each sample\n",
    "    action_count = torch.zeros((state_size, action_size))#.to(device)\n",
    "    total_reward = torch.zeros((1))#.to(device)\n",
    "    while True:\n",
    "        action = np.random.choice(action_size, p=policy[state])\n",
    "        action_count[state, action] += 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, action_count\n",
    "\n",
    "def update_policy_tensor(policy, samp_tensor, rew_tensor, learn_rate,\n",
    "                         smooth_factor, action_size, keep, device):\n",
    "    # sort tensor by reward and return indices of best performers to samp_index\n",
    "    samp_index = rew_tensor.sort(descending=True)[1]\n",
    "    # get indices of best samples\n",
    "    samp_index = samp_index[:keep]\n",
    "    # only keep best samples by using samp_index and index_select\n",
    "    samp_tensor = samp_tensor.index_select(0, samp_index)\n",
    "    # sum the results to get the action counts by state and action\n",
    "    samp_tensor = samp_tensor.sum(dim=0)\n",
    "    # sum to get action counts by state\n",
    "    action_count = samp_tensor.sum(dim=1)\n",
    "    # only want states visited at least once\n",
    "    mask = action_count.ge(0.5)\n",
    "    # update policy with best samples\n",
    "    policy[mask] = samp_tensor[mask]/action_count[mask,None]*learn_rate + policy[mask]*(1.-learn_rate)\n",
    "    # normalize policy so that they sum to 1\n",
    "    policy = policy / policy.sum(dim=1)[:,None]\n",
    "     \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tensor = torch.ones((state_size,action_size)).to(device)/action_size\n",
    "\n",
    "for t in range(trial):\n",
    "    sample_tensor = torch.zeros((samples, state_size, action_size)).to(device)\n",
    "    reward_tensor = torch.zeros((samples)).to(device)\n",
    "    policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "    for s in range(samples):\n",
    "        reward, action_table_tensor = run_sample_tensor(env, policy_array, \n",
    "                                           state_size, action_size, device)\n",
    "        reward_tensor[s] = reward\n",
    "        sample_tensor[s] = action_table_tensor \n",
    "    policy_tensor = update_policy_tensor(policy_tensor, sample_tensor, reward_tensor, \n",
    "                         learning_rate, smoothing_factor, action_size, keep_best, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 1.0 Average Length: 6.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        # choose action based on best action in that state\n",
    "        action = np.argmax(policy_array[state])\n",
    "        # choose action based policy distribution\n",
    "        #action = np.random.choice(action_size,p=policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake Slippery using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "smoothing_factor = 1\n",
    "trial = 1000\n",
    "keep_best = int(0.2*samples)\n",
    "\n",
    "policy_tensor = torch.ones((state_size,action_size)).to(device)/action_size\n",
    "\n",
    "for t in range(trial):\n",
    "    sample_tensor = torch.zeros((samples, state_size, action_size)).to(device)\n",
    "    reward_tensor = torch.zeros((samples)).to(device)\n",
    "    policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "    for s in range(samples):\n",
    "        reward, action_table_tensor = run_sample_tensor(env, policy_array, \n",
    "                                           state_size, action_size, device)\n",
    "        reward_tensor[s] = reward\n",
    "        sample_tensor[s] = action_table_tensor \n",
    "    policy_tensor = update_policy_tensor(policy_tensor, sample_tensor, reward_tensor, \n",
    "                         learning_rate, smoothing_factor, action_size, keep_best, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 0.59 Average Length: 36.88\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        # choose action based on best action in that state\n",
    "        action = np.argmax(policy_array[state])\n",
    "        # choose action based policy distribution\n",
    "        #action = np.random.choice(action_size,p=policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('Taxi-v2')\n",
    "state_size = env.nS\n",
    "action_size = env.nA\n",
    "\n",
    "# hyperparameters\n",
    "samples = 100\n",
    "learning_rate = 0.1\n",
    "smoothing_factor = 1\n",
    "trial = 1000\n",
    "keep_best = int(0.5*samples)\n",
    "\n",
    "policy_tensor = torch.ones((state_size,action_size)).to(device)/action_size\n",
    "\n",
    "for t in range(trial):\n",
    "    sample_tensor = torch.zeros((samples, state_size, action_size)).to(device)\n",
    "    reward_tensor = torch.zeros((samples)).to(device)\n",
    "    policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "    for s in range(samples):\n",
    "        reward, action_table_tensor = run_sample_tensor(env, policy_array, \n",
    "                                           state_size, action_size, device)\n",
    "        reward_tensor[s] = reward\n",
    "        sample_tensor[s] = action_table_tensor \n",
    "    policy_tensor = update_policy_tensor(policy_tensor, sample_tensor, reward_tensor, \n",
    "                         learning_rate, smoothing_factor, action_size, keep_best, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 8.23 Average Length: 12.77\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using the found policy\n",
    "episodes = 100\n",
    "episode_reward_list, episode_len_list = [], []\n",
    "policy_array = policy_tensor.cpu().numpy()\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        # choose action based on best action in that state\n",
    "        action = np.argmax(policy_array[state])\n",
    "        # choose action based policy distribution\n",
    "        #action = np.random.choice(action_size,p=policy_array[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            episode_reward_list.append(episode_reward)\n",
    "            episode_len_list.append(episode_length)\n",
    "            #print(\"Episode {}: Reward: {} Length: {}\".format(i, episode_reward, episode_length))\n",
    "            break\n",
    "    \n",
    "print(\"Average Reward: {} Average Length: {}\".format(np.mean(episode_reward_list), np.mean(episode_len_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_pytorch",
   "language": "python",
   "name": "rl_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

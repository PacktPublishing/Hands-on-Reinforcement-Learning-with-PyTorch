{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"1.6 Understanding OpenAI Gym Environments.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_5SV8FqkjAot","colab_type":"code","colab":{}},"source":["# import OpenAI gym\n","import gym\n","\n","# create the environment\n","env = gym.make('CartPole-v0')\n","\n","# initialize the env\n","env.reset()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZcSh0OJjAoy","colab_type":"text"},"source":["### Display CartPole"]},{"cell_type":"code","metadata":{"id":"VJPUpslZjAoz","colab_type":"code","colab":{}},"source":["# show a few frames of CartPole\n","for i in range(100):\n","    # display the env (optional)\n","    # env.render()\n","    # randomly chose an action from all available actions\n","    action = env.action_space.sample()\n","    # agent takes an action and interacts with the env, receiving state, reward, done and info\n","    state, reward, done, info = env.step(action)\n","    # if episode is over reset the env\n","    if done:\n","        env.reset()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6BNrKZKjAo3","colab_type":"text"},"source":["### The Reinforcement Learning Loop"]},{"cell_type":"code","metadata":{"id":"ZZTUa79SjAo4","colab_type":"code","outputId":"89ed6c1f-780b-46fe-b3fd-d519287508f5","colab":{}},"source":["episodes = 10\n","\n","# run environment for 10 episodes\n","for ep in range(episodes):\n","    episode_reward = 0\n","    while True:\n","        # randomly chose an action from all available actions\n","        action = env.action_space.sample()\n","        # agent takes an action and interacts with the env, receiving state, reward, done and info\n","        state, reward, done, info = env.step(action)\n","        episode_reward += 1\n","        # if episode is over reset the env\n","        if done:\n","            print(\"Episode {} done with reward: {}\".format(ep, episode_reward))\n","            env.reset()\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 0 done with reward: 25\n","Episode 1 done with reward: 10\n","Episode 2 done with reward: 103\n","Episode 3 done with reward: 24\n","Episode 4 done with reward: 14\n","Episode 5 done with reward: 18\n","Episode 6 done with reward: 37\n","Episode 7 done with reward: 48\n","Episode 8 done with reward: 15\n","Episode 9 done with reward: 11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dVrPNJyljAo9","colab_type":"text"},"source":["### Episodes and Timesteps"]},{"cell_type":"code","metadata":{"id":"-lQ9hFdejAo-","colab_type":"code","outputId":"c4f5db15-06c7-4025-f64c-953f5af899ad","colab":{}},"source":["episodes = 10\n","max_timesteps = 200\n","# run environment for 10 episodes\n","for ep in range(episodes):\n","    timestep = 0\n","    while timestep < max_timesteps:\n","        # randomly chose an action from all available actions\n","        action = env.action_space.sample()\n","        # agent takes an action and interacts with the env, receiving state, reward, done and info\n","        state, reward, done, info = env.step(action)\n","        timestep += 1\n","        # if episode is over reset the env\n","        if done:\n","            print(\"Episode {} done after {} timesteps\".format(ep, timestep))\n","            env.reset()\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 0 done after 15 timesteps\n","Episode 1 done after 12 timesteps\n","Episode 2 done after 23 timesteps\n","Episode 3 done after 16 timesteps\n","Episode 4 done after 18 timesteps\n","Episode 5 done after 14 timesteps\n","Episode 6 done after 29 timesteps\n","Episode 7 done after 33 timesteps\n","Episode 8 done after 42 timesteps\n","Episode 9 done after 68 timesteps\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tSL7bwx_jApC","colab_type":"text"},"source":["### Interacting with the Environment: actions, done and env.step()"]},{"cell_type":"code","metadata":{"id":"UuH0Acv_jApC","colab_type":"code","outputId":"fb4d2d0c-4810-4729-b788-f19d3ed37e0d","colab":{}},"source":["episodes = 1\n","\n","max_timesteps = 20\n","\n","for ep in range(episodes):\n","    timestep = 0\n","    while timestep < max_timesteps:\n","        # randomly chose an action from all available actions\n","        action = env.action_space.sample()\n","        # agent takes an actiona nd interacts with the env, receiving state, reward, done and info\n","        state, reward, done, info = env.step(action)\n","        timestep += 1\n","        print(\"Timestep {}: agent took action {}\".format(timestep, action))\n","        print(\"Timestep {}: state {}, reward {}, done {}, info {}\".format(timestep, state, reward, done, info))\n","        # if episode is over reset the env\n","        if done:\n","            env.reset()\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Timestep 1: agent took action 0\n","Timestep 1: state [0.04505125 0.00220196 0.01351047 0.00187966], reward 1.0, done False, info {}\n","Timestep 2: agent took action 1\n","Timestep 2: state [ 0.04509529  0.19712758  0.01354806 -0.28651012], reward 1.0, done False, info {}\n","Timestep 3: agent took action 0\n","Timestep 3: state [0.04903784 0.00181506 0.00781786 0.01041478], reward 1.0, done False, info {}\n","Timestep 4: agent took action 0\n","Timestep 4: state [ 0.04907414 -0.19341814  0.00802616  0.30555405], reward 1.0, done False, info {}\n","Timestep 5: agent took action 1\n","Timestep 5: state [0.04520578 0.00158853 0.01413724 0.01541314], reward 1.0, done False, info {}\n","Timestep 6: agent took action 1\n","Timestep 6: state [ 0.04523755  0.19650491  0.0144455  -0.272776  ], reward 1.0, done False, info {}\n","Timestep 7: agent took action 1\n","Timestep 7: state [ 0.04916764  0.39141779  0.00898998 -0.56086799], reward 1.0, done False, info {}\n","Timestep 8: agent took action 1\n","Timestep 8: state [ 0.056996    0.58641243 -0.00222738 -0.85070511], reward 1.0, done False, info {}\n","Timestep 9: agent took action 1\n","Timestep 9: state [ 0.06872425  0.78156468 -0.01924148 -1.14408761], reward 1.0, done False, info {}\n","Timestep 10: agent took action 1\n","Timestep 10: state [ 0.08435554  0.97693265 -0.04212323 -1.44274197], reward 1.0, done False, info {}\n","Timestep 11: agent took action 0\n","Timestep 11: state [ 0.1038942   0.78235382 -0.07097807 -1.1635132 ], reward 1.0, done False, info {}\n","Timestep 12: agent took action 1\n","Timestep 12: state [ 0.11954127  0.97832451 -0.09424834 -1.47757911], reward 1.0, done False, info {}\n","Timestep 13: agent took action 1\n","Timestep 13: state [ 0.13910776  1.17446257 -0.12379992 -1.7981485 ], reward 1.0, done False, info {}\n","Timestep 14: agent took action 1\n","Timestep 14: state [ 0.16259701  1.3707334  -0.15976289 -2.1266058 ], reward 1.0, done False, info {}\n","Timestep 15: agent took action 1\n","Timestep 15: state [ 0.19001168  1.56704237 -0.20229501 -2.46409   ], reward 1.0, done False, info {}\n","Timestep 16: agent took action 1\n","Timestep 16: state [ 0.22135253  1.7632174  -0.25157681 -2.81142187], reward 1.0, done True, info {}\n"],"name":"stdout"}]}]}